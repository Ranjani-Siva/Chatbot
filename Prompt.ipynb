{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BPGLRFEhgsNrwvaAaoTixjcGqCcuUTk0",
      "authorship_tag": "ABX9TyNtWefJCw5u3+zkuR3VIIOh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani-Siva/Chatbot/blob/main/Prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zWiMB8ZUwej",
        "outputId": "774713e1-a0cf-4720-fc3b-86e858ecf812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n",
        "# Using punkt Tokenizer\n",
        "nltk.download('punkt')\n",
        "# Using wordnet Dictionary\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the text file from drive\n",
        "f = open('/content/drive/MyDrive/Da basic/Me/Chatgpt data of dm/tadata.txt','r', errors= 'ignore')\n",
        "data=f.read()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "XamPXTbuFPm3",
        "outputId": "e3eae23e-b9f1-497c-c15a-8105c15feded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data mining is the analysis step of knowledge discovery in databases process, or KDD. \\nData mining is the extraction of hidden predictive information from large databases is a new technology with great potential to help companies focus on the most important information in their data warehouses.\\n\\n–\\tIt’s Non-trivial extraction of implicit, previously unknown and potentially useful information from data\\n–\\tExploration & analysis,by automatic orsemi-automatic means, of large quantities of data in order to discover meaningful patterns\\n\\n\\nData Mining is defined as the procedure to extracting information from huge sets of data. we can say that data mining is mining knowledge from data. Huge volume of data is available in the Information Industry. This kind of data is no use until it is converted into useful information, It is necessary to analyze the huge volume of data and extract useful information from it. In Extraction, the information is not the only process we need to perform, it also involves other processes such as Data Cleaning, Data Integration, Data Transformation, Data Mining, Pattern Evaluation and Data Presentation. When all these processes are over we would be able to use this information in many applications such as Fraud Detection, Market Analysis, Production Control, Science Exploration.\\n\\nData mining is the process of discovering patterns from large data sets involving methods at the intersection of machine learning, statistics and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information from a data set and transform the information into a comprehensible structure for further use. Apart from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post- processing of discovered structures, visualization, and online updating.\\n\\nThe term \"data mining\" is, in fact, a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing as well as any application of computer decision support system, including artificial intelligence and business intelligence. Practical machine learning tools and techniques with Java was original to be named just Practical machine learning and the term data mining was only added for marketing reasons. Often the more\\n general terms data analysis and analytics, when referring to actual methods, artificial intelligence, and machine learning are more appropriate.\\nThe actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records, unusual records and dependencies. This usually involves using database techniques such as spatial indices. These patterns can be seen as a kind of summary of the input data and may be used in the further analysis, for example, in machine learning and predictive analytics. the data mining step might identify multiple groups in the data, which can be used to obtain more accurate prediction results by a decision support system. The data collection, data preparation or result interpretation and reporting is part of the data mining step.\\nThe related terms data dredging, data fishing and data snooping refer to the use of data mining methods to sample parts of a larger data set that are too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can be used in creating new hypotheses for testing against the larger data populations.\\n\\nHistory of data mining\\nIn the 1960s, analyst and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an appropriate hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies 1983. Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" to \"fishing\" or \"snooping\"\\nThe term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in the 1980s, a phrase \"database mining\"™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation researchers, consequently turned to data mining. Other terms used include data geographer, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. the term data mining became more popular in the business and press communities and the terms data mining and knowledge discovery are used interchangeably.\\nIn the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. After a year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in- chief. Later he started the SIGKDD Newsletter & SIGKDD Explorations. The KDD International conference became the main highest quality conference in data mining\\n with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.\\nThe manual extraction in the patterns for data has occurred for centuries. Early methods of identifying patterns in data include Bayes\\' theorem in the 1700s and regression analysis at 1800s. The boost, ubiquity and increasing power of computer technology have dramatically increased data collection, storage and manipulation ability. The data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, such as neural networks, cluster analysis, genetic algorithms in the 1950s, decision trees and decision rules in the 1960s, and support vector machines in the 1990s. Data mining is the process of applying all these methods with the intention of uncovering hidden patterns in large data sets. It bridges the gap from applied statistics and artificial intelligence which usually provides the mathematical background to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms in more efficiently, allowing such methods to be applied to ever larger data sets.\\n\\n\\nThe KDD process is commonly defined with these stages:\\n1.\\tSelection\\n2.\\tPre-processing\\n3.\\tTransformation\\n4.\\tData mining\\n5.\\tInterpretation/evaluation.\\nData Selection\\nData mining process requires a large volume of historical data for analysis. So that the data repository with integrated data contains much more data than actually required. From the available data, the data needs to be selected and stored. From the Data, the selection is a process where the data relevant to the analysis is retrieved from the database.\\nData Preprocessing\\nData pre-processing is data mining techniques that involve transforming the raw data into an understandable format. In the Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method for resolving such issues. Data preprocessing prepare raw data for further processing. Data preprocessing is used the database as driven applications such as customer relationship management and rule-based applications\\nData Transformation\\nData transformation is the process of transforming and consolidating the data into different forms that are suitable for data mining. Data transformation normally involves normalization, aggregation, generalization etc. For example, a data set available as\\n\"-5, 37, 100, 89, 78\" can be transformed as \"-0.05, 0.37, 1.00, 0.89, 0.78\". Here data becomes more suitable for data mining. After data integration, the available data is ready for data mining.\\nData Mining\\nData mining is the core process where a number of complex and intelligent methods are applied to extract patterns from the data set. In the Data mining process, it includes a number of tasks such as association, classification, prediction, clustering, time series analysis and so on.\\nEvaluation\\nThe pattern evaluation will identify the truly interesting patterns representing knowledge based on different types of interesting measures. A pattern is considered to be interesting if it is potentially useful and it can be easily understandable by humans, validates some hypothesis that someone wants to confirm or valid on new data with some degree of certainty.\\n\\nThe process will exist in the Cross-Industry Standard Process for Data Mining (CRISP- DM) which defines six phases:\\n1.\\tBusiness understanding\\n2.\\tData understanding\\n3.\\tData preparation\\n4.\\tModeling\\n5.\\tEvaluation\\n6.\\tDeployment\\n\\nA simplified process such as\\n(1)\\tPre-processing,\\n(2)\\tData Mining,\\n(3)\\tResults Validation.\\nPolls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named on these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.\\n \\nPre-processing\\nBefore the data mining algorithms were used, a target data set must be assembled. The data mining can only uncover patterns actually present in the data and the target data set must be large enough to contain these patterns, while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining were used. The target is set and then cleaned. Data cleaning will remove the observations containing noise and those with missing data.\\nData Mining Task\\nData mining involves six common classes of tasks\\n•\\tAnomaly detection (outlier/change/deviation detection) – The identification of unusual data records, that might be interesting or data errors that require further investigation.\\n•\\tAssociation rule learning (dependency modeling) – Searches for relationships between variables. For example, a supermarket might gathering the data from their customer purchasing habits Using association rule learning, the supermarket can determine which products are frequently bought together and use this information for marketing purposes. This is sometimes referred as market basket analysis.\\n•\\tClustering – is the task of discovering groups and structures in the data that are in some way or another \"similar\", without using known structures in the data.\\n•\\tClassification – is the task of generalizing known structure to apply to new data. For example, an e-mail program might attempt to classify an e-mail as \"legitimate\" or as \"spam\".\\n•\\tRegression – attempts to find a function which models the data with the least error that is, for estimating the relationships among data or datasets.\\n•\\tSummarization – providing a more compact representation of the data set, including visualization and report generation.\\nResults validation:\\nData mining can accidentally be misused, and can then produce results which appear to be significant, but which do not actually predict future behavior and cannot be reproduced on a new sample of data and move little use. Often this results from exploring too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this kind of problem in machine learning is known as over fitting, but the same problem can arise at different phases of the process and thus a train or test split - when applicable at all - may not be sufficient to prevent this from happening.\\nThe final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms which occur in the wider data set. Not all patterns found by the data mining algorithms are necessarily a valid one. It is common for the data mining algorithms to find patterns from the training set which are not present in the general data set. It is called over fitting. To overcome the over fitting, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied from the test set, and the resulting output is compared to the desired output. For instance, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. A number of statistical methods may be used  to evaluate the algorithm, such as ROC curves.\\nIf the learned patterns do not meet the desired standards, subsequently it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns to meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\\n\\nThere is a wide range of data mining techniques (algorithms), which can be applied in all kinds of domains where data has to be analyzed. Some example of data mining applications are:\\n•\\tfraud detection,\\n•\\tstock market price prediction,\\n•\\tanalyzing the behavior of customers in terms of what they buy In general data mining techniques are chosen based on:\\n•\\tthe type of data to be analyzed,\\n•\\tthe type of knowledge or patterns to be extracted from the data, \\n\\nTo perform data mining, there are many software programs available in this world. Some of them are general purpose tools offering many algorithms of different kinds, while others are more specialized. Also, some software programs are commercial, while others are open-source.\\nThe SPMF open-source data mining library, which is free and open-source, and specialized in discovering patterns from data. But there are many other popular software such as Weka, Knime, RapidMiner, and the R language, to name a few.\\nIntroduction to Data Warehousing\\nThe idea of data warehousing evolved during late 1980s. The term data warehouse was coined by William H. Inmon who is known as the Father of Data Warehousing.\\nThis concept is used to provide an architectural model for the flow of data from operational systems to various decision support environments.\\nIn This concept attempts to brief a variety of problems which is very closer associated with this flow mainly the high costs associated with it. In the absence of a data warehousing architecture, a great amount of redundancy is required to support multiple decision support environments.\\n\\nIn larger scenarios, it is typical for multiple decision support environments to operate independently. Though each environment serves different users, they often require the same stored data.\\nThe process of gathering, cleaning and integrating data for example from various sources, usually from long-term existing operational systems is in part for each environment. The operational systems are frequently reexamined as new decision support a requirement emerges. Often new requirements require gathering, cleaning and integrating new data from \"data marts\" which is tailored for instant access by users.\\n\\nData warehouse which is also known as an enterprise data warehouse is a system used for reporting and data analysis. It is also considered as a core component of business intelligence. The data stored in the warehouse is uploaded from the operational systems. The data passes through an operational data store and will require data cleansing for additional operations to ensure data quality before it is used in the Data Warehouse for reporting.\\nThe typical Extract, transform, load (ETL)- based data warehouse uses staging, data integration, and access layers to express its key functions. The staging layer database stores raw data taken from each of the disparate source data systems.\\nThe integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database.\\nThe integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups and into facts and aggregate facts.\\nStar schema is the name coined for the combination of facts and dimensions the users can retrieve data through access layer.\\nThe main source of the data is cleansed, transformed, catalogued and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support.\\nThe method to retrieve and analyze data, to extract, transforms, and load data, and to manage the data dictionary are also important for a data warehousing system.\\nAn expanded definition for data warehousing includes business intelligence tools, tools to extract, transform and load data into the repository and tools to manage and retrieve metadata.\\n\\nData warehousing emphasizes the capture of data from data mart diverse sources for useful analysis and access start from the point-of-view of the end user who may need access to specialized, sometimes local databases.\\n\\nTwo approaches in data warehousing top down and bottom up.\\nThe top down approaches spins off data marts for specific groups of users after the complete data warehouse has been created.\\nThe bottom up approach builds the data marts first and then combines them into a single all-encompassing data warehouse.\\n\\nA data warehouse is housed on an enterprise mainframe server, in the cloud. Data from various online transaction processing (OLTP) applications and other sources is selectively extracted.\\nWilliam H. Inmon described a data warehouse as being a subject-oriented, integrated, time-variant and nonvolatile collection of data which supports management\\'s decision- making process.\\nIt is also known as technique for collecting and managing data from variety of sources to provide meaningful business insights. It is a cluster of technologies and components that allow the strategic use of data.\\nIt is an electronic storage of a large amount of information by a business which is designed for query and analysis. It is a process of transforming data into information and making it available to users.\\nThe decision support database (Data Warehouse) is maintained in different method from the organization’s operational database. Many people mistake data warehouse to be a product which actually is an environment.\\n \\nHistorical and current decision there is actually very much difficult for accessing the traditional operational data store\\nThe data warehouse is the very much utilized for data analysis and reporting is the crux of the business intelligent system.\\nThe 3NF database has interconnected tables related to one and other. For an instance, current inventory information consists of 12 or more joined conditions which actually can deteriorate the speed of the response to the report and query.\\nOn the other hand the data warehouse offers a peculiar design which can help to increase the probability of queries for analytics and report. In addition to it the data warehouse decrease the response time also.\\n\\nHistory of Data warehouse\\nMain benefits of data warehouse are it helps the users to understand, enhance and improve the performance as for as their organization is concerned. As computer system started becoming more complex and handling the amount of information becomes harder the need and importance of data warehouse emerged.\\nBut data warehousing is not something new. For example 1960- Dartmouth and General Mills jointly coined the definition dimensions and facts. Dimensional data marts for retail sales was introduces in the year 1970 by Nielsen.\\nA company called Tera Data Corporation in 1983- introduced a specifically designed data base management system for decision support.\\nIBM worker Paul Murphy and Barry Devlin developed the Business Data Warehouse the late 1980s and from then on data warehousing started .on the contrary the master mind behind data warehousing is Inmon Bill he is considered the father of data warehouse. He has considered his writing about maintenance of the warehouse and the corporate information factory building and usage\\n\\nHow Data warehouse works\\nA Data Warehouse is the power house where the information is received from many data sources. Data is received into a data warehouse from other data bases. Data is classified into 3 different sections which include\\n1.\\tStructured\\n2.\\tSemi-structured\\n3.\\tUnstructured data.\\n\\nThe data is inhaled, transfigured and processed   and thereby allowing the user to access the data which is processed via, SQL clients, and spreadsheets. In addition to \\nthis information is joined from different sources through data warehouse into a single comprehensive database.\\nThrough this the customer can accurately analyze because all the information is merged in on place. This process also ensures that the data and information is considered by the data warehouse properly. Another benefit of data warehousing is it allow data mining which actually lead to better sales and higher profits through the pattern system.\\n\\nTypes of Data Warehouse\\n1. Enterprise Data Warehouse:\\nIt offers decision support services throughout the Enterprise. It is also a centralized warehouse. It provides a uniformed and unique approach in representing and organizing data.   Enterprise data ware house helps the user to select identify and access based on division and differentiate data based on the subject .\\n2. Operational Data Store:\\nOperational Data Store operates as a substitute which neither OLTP nor data warehouse support reporting. Data warehouse is refreshed from time to time in operational data store. Therefore operational data store is usually used for regular activities which include storage of employee’s record. .\\n3. Data Mart:\\nData Mart is exclusively designed for the business which includes finance and sales. Data mart is the subdivision of data warehouse.\\n\\nStages of Data Warehouse Offline Operational Database:\\nIn this data base the data is transformed from operational system to a server. Processing, reporting and loading of the copied data hover impact the performance of the operational systems\\nOffline Data Warehouse:\\nIn this Data warehouse data is frequently updated from the Operational Database. The transformation of data in the Data warehouse is done to meet the objectives of Data warehouse\\nreal time data warehouse:\\nduring this stage every data warehouses are refreshed and updated during a transaction.\\nReal time Data Warehouse:\\nDuring this stage every data warehouses are refreshed and updated during a transaction. For instance this is done when a ticket is booked in an airline or in a railway booking system.\\nIntegrated Data Warehouse:\\nThis stage leads to the next process that is when an transaction takes place. The transaction is generated through the Data Warehouses to the operational system\\nComponents of Data warehouse Load manager:\\nIt extract an input data into the warehouse its otherwise called as the front component. It also transforms the preparation of data to enter in to a data\\nData Warehouse Manager:\\nWarehouse manager does operation which is closely associated with managing the data in the warehouse. It analysis data to conform creation of induces and view consistency, de-normalization and aggregations\\nQuery Manager:\\nIt is very well known as backend component. It assists the entire operation operations related to the user queries..\\n\\nEnd-user access tools:\\nThis includes five different groups like\\n1. Data Reporting\\n2. Query Tools\\n3. Application development tools\\n4. EIS tools,\\n5. OLAP tools and data mining tools.\\n\\nNeed of data warehouse\\n• User who need mass amount of data\\n• It helps\\tuser who obtain customized difficult processes to receive information from various data source\\n• It benefits people who demand basic technology to access the data\\n• Users who requires a perfect approach to taking decisions\\n• For users who require fast performance on a huge amount of data which is a necessary for obtaining charts grids an reports\\n• Users who want to identify the hidden patterns of grouping and data flows\\n \\nData Warehouse Used For\\nHere, are most common sectors where Data warehouse is used: \\nAirline:\\nIt is used for root portability frequent flyer promotions crue assignment analyzes banking for better desk effective management identifying the resource and managing it\\nCertain bank use data warehouse for performance analyses an market research Healthcare:\\nData warehouse is used to generate patient’s disease reports, treatment outcome share data with the medical aid services, and insurance company.\\nPublic sector:\\nIt is used in government sector to properly analysis the details of very individuals health records tax record. It is also used for intelligence gathering\\nInvestment and insurance sector\\nThis sector involves analyzation of data patterns customer trends and is also used to track market movement and fluctuation.\\nRetail chain:\\nIn retail chains, Data warehouse promptly benefits for the marketing and distributing process.\\nIt is also helpful to promote product and determining price issues, track customer buying pattern and items. This is widely used for distribution and marketing.\\nIt also helps to track items, customer buying pattern, promotions and also used for determining pricing policy.\\nTelecommunication:\\nA data warehouse is very much used in this sector to implement distribution decisions sale decision and product promotion\\nHospitality Industry:\\nThis Industry makes use of data warehousing\\tto estimate and design,\\tto promote campaigns and advertising\\n\\nSteps to Implement Data Warehouse\\nData warehouse implementation is to employ a three-prong strategy as below\\n1. Enterprise strategy:\\nData transformation and data mapping is fast through this strategy, and at the same time attributes, dimension and fax are identified\\n2. Phased delivery:\\nData warehouse implementation is phased based upon subject areas. For instance business steps like booking and billing are first implemented and then integrated with one another.\\n3. Iterative Prototyping:\\nIn this method data ware house is developed and tested iteratively \\n\\nBest practices to implement a Data Warehouse\\n• First decide a plan to accurately identify the consistency and integrity of the data.\\n• The data warehouse should be, well defined and well integrated\\n• While designing Data warehouse make sure you use right tool, stick to life cycle, take care about data conflicts and ready to learn from your mistakes.\\n• Don’t replace reports and operational systems\\n• When designing data warehouse, conform that the right tool is\\tbeing used and don’t spend much time on cleaning extracting and loading data.\\n• Identify and implement a training plan for the end users\\n• Try to involve all stack holder and business partners in data warehouse implantation process.\\n\\nData Warehouse- Advantages & Disadvantages \\nAdvantages:\\n• It permits the user to gather and access critical data from sources and put them together in a place\\n• Data warehouse give constant information about different cross functional performance. It also supports ad-hoc query and reporting\\n• Data warehouse allows the user to integrate and restructure which allows the user to easily analyze an report\\n• Data warehouse helps the user to be time effective in analyzing and reporting. it also help the user to access many important data from various sources by a single touch \\n• Data warehouse helps the user to predict the upcoming future as its stores the large amount of historical data.\\n\\nDisadvantages:\\n• It is not advisable option for unstructured data\\n• The implementation process of data ware house is time consuming process\\n• The beginner finds too difficult to use the data ware house .at the same time data warehouse can be outdate very quickly\\n• It is very much difficult to make few changes in quires indecs data sources scheme and data typos and ranges\\n\\nThe Future of Data Warehousing\\nIn spite of all its negatives and short coming the scope of data war housing are on the raise as the regulatory constraints changes from time to time. It may limit the performance to combine different data from difference sources.\\nThis may include unstructured data which is usually a difficult one to store.\\nThis is because the size of the data grows everyday data warehouse system helps the user to build and run data as a size grows\\n\\nData Warehouse Tools\\nThere are many Data Warehousing tools are available in the market.\\nOnline Analytical Processing Server\\nOnline Analytical Processing Server (OLAP) is based on multidimensional data models. It allows managers and analysts to get a piece of deep information through fast, consistent, and interactive access to information. \\n\\nTypes of OLAP Servers\\nOLAP had four types of servers they are listed below\\n• Relational OLAP (ROLAP)\\n• Multidimensional OLAP (MOLAP)\\n• Hybrid OLAP (HOLAP)\\n• Specialized SQL Servers \\nRelational OLAP\\nROLAP is Relational Online Analytical Processing model where the data is stored in a relational database. for eg rows and columns in the data warehouse. In the ROLAP model data was present in front of the user in the multidimensional form. To display the data, in multidimensional views a semantic layer of metadata is created that maps dimension to the relational tables. Metadata also supports the aggregation of the data.\\nWhenever the ROLAP engine gets analytical server issues a complex query, it fetches data from the main warehouse and dynamically creates a multidimensional view of data for the user and it differs from MOLAP because MOLAP already has a static multidimensional view of data stored in proprietary databases MDDBs.\\nAs the multidimensional view of data is created dynamically it processes slower in comparison to MOLAP. ROLAP engine deals with large volumes of data.\\nRelational online analytical processing (ROLAP) is a form of online analytical processing (OLAP) that performs dynamic multidimensional analysis of data stored in a relational database rather than in a multidimensional database which is usually considered the OLAP standard. ROLAP uses a relational database and it requires more processing time or disk space to perform some of the tasks that multidimensional databases are designed for. However, ROLAP supports larger user groups and greater amounts of data and is often used when these capacities are crucial, such as in a large and complex department of an enterprise. Relational OLAP servers are placed between relational back-end server and client front-end tools to store and manage the warehouse data and the relational OLAP uses or extended-relational Data Base Management System. ROLAP includes\\n• Implementation of aggregation navigation logic\\n• Optimization for each DBMS back-end\\n• Additional tools and services.\\n\\nAdvantages of ROLAP\\n• ROLAP servers can be easily used with existing Relational Data Base Management System.\\n• Data can be stored efficiently, and no zero facts can be stored.\\n• ROLAP tools do not use pre-calculated data cubes.\\n• DSS server of micro-strategy adopts the ROLAP approach.\\n\\nDisadvantages of ROLAP\\n• Poor query performance.\\n• Some limitations of scalability depending on the technology architecture that is utilized.\\n\\nMultidimensional OLAP\\nMOLAP is a Multidimensional Online Analytical Processing model. The data was used to analyze and is stored in specialized multidimensional databases (MDDBs). The multidimensional database management systems are proprietary software systems.\\nThese multidimensional databases are formed from the large multidimensional array. The cells or data cubes of this multidimensional databases will carry pre-calculated and prefabricated data. The Proprietary software systems create this pre-calculated and fabricated data, while the data is loaded to MDDBs from the main databases.\\nNow, it is the work of MOLAP engine, it will reside there in the application layer and provide the multidimensional view of data from the Multidimensional Database to the user. Thus when a user request for the data, no time is wasted in calculating the data and the system responses fast. It is the more traditional way of OLAP analysis. In MOLAP, data is stored in a multidimensional cube. The storage is not in the relational storage.\\nDatabase but in proprietary formats. Multidimensional OLAP (MOLAP) uses array-based multidimensional storage engines for multidimensional views of data with multidimensional data storage, the storage utilization may be low if the dataset is sparse. Most of the MOLAP servers use two levels of data storage representation to handle dense and sparse datasets.\\n\\nAdvantages of OLAP\\n• MOLAP allows the fastest indexing to the pre-computed summarized data.\\n• MOLAP cubes are built for fast data retrieval and they are optimal for slicing and dicing operations.\\n• MOLAP can perform complex calculations: All calculations have been pre- generated when the cube was created. so complex calculations are not only doable, but they return quickly.\\n• MOLAP Helps the users to connect a network who need to analyze larger and less-defined data.\\n• It is easier and suitable for inexperienced users Disadvantages\\n• MOLAP is not capable of containing details of data.\\n• The storage of utilization may be low if the data set is sparse.\\n \\nDifferences between ROLAP and MOLAP\\n1. ROLAP stands for Relational Online Analytical Processing whereas; MOLAP stands for Multidimensional Online Analytical Processing.\\n2. In both the cases, ROLAP and MOLAP data is stored in the main warehouse. In ROLAP data is directly fetched from the main warehouse whereas, in MOLAP data is fetched from the proprietary databases MDDBs.\\n3. In ROLAP, data is stored in the form of relational tables but, in MOLAP data is stored in the form of a multidimensional array made of data cubes.\\n4. ROLAP deals with large volumes of data whereas, MOLAP deals with limited data summaries kept in MDDBs.\\n5. ROLAP engines use complex SQL to fetch data from the data warehouse. However, MOLAP engine creates prefabricated and pre-calculated data cubes to present multidimensional view of data to a user and to manage data sparsity in data cubes, MOLAP uses sparse matrix technology.\\n6. ROLAP engine creates a multidimensional view of data dynamically whereas, MOLAP statically stores multidimensional view of data in proprietary databases MDDBs for a user to view it from there.\\n7. As ROLAP creates a multidimensional view of data dynamically, it is slower than MOLAP which do not waste time in creating a multidimensional view of data.\\n\\nHybrid OLAP\\nHybrid online analytical processing (HOLAP) is a combination of relational OLAP (ROLAP) and multidimensional OLAP (usually referred to simply as OLAP). HOLAP was developed to combine the greater data capacity of ROLAP with the superior processing capability of the OLAP.\\nHOLAP can use varying combinations of ROLAP and OLAP technology. Typically it stores data in both a relational database (RDB) and a multidimensional database (MDDB) and uses whichever one is best suited to the type of processing desired. The databases are used to store data in the most functional way. For data-heavy processing, the data is more efficiently stored in an RDB, while for speculative processing, the data is more effectively stored in an MDDB.\\nHOLAP user can choose to store the results of their queries to the MDDB to save the effort of looking for the same data over and over which saves time. This technique is called \"materializing cells\". It improves performance and it takes a toll on storage. The user has to strike the balance between performance and storage demand to get the most out of HOLAP but it offers the best features of both OLAP and ROLAP, HOLAP is increasingly preferred.\\nHOLAP technologies were attempting to combine the advantages of MOLAP and ROLAP. For summary type information\\'s HOLAP leverages cube technology for faster performance. When detail information was needed, HOLAP can \"drill through\" from the cube into the underlying relational data. It offers scalability of ROLAP and faster computation of MOLAP. HOLAP servers allow storing the large data volumes of detailed information. The aggregations are stored separately in the MOLAP store.\\n\\nSpecialized SQL Servers\\nSpecialized SQL servers provide advanced query language and query processing support for SQL queries over star and snowflake schemas in a read-only environment.\\n\\nOLAP Operations\\nOLAP servers were based on the multidimensional view of data and we will discuss OLAP operations in multidimensional data.\\nHere is the list of OLAP operations −\\n• Roll-up\\n• Drill-down\\n• Slice and dice\\n• Pivot\\nRoll-up\\nRoll-up performs aggregation on a data cube in any of the following ways −\\n• By climbing up a concept hierarchy for a dimension\\n• By dimension reduction.\\n• Roll-up is performed by climbing up to the concept hierarchy for the dimension location.\\n• Initially the concept hierarchy was \"street < city < province < country\".\\n• On rolling up, the data is aggregated by ascending the location hierarchy from the level of the city to the level of the country.\\n• The data is grouped into cities rather than the countries.\\n• When roll-up is performed by one or more dimensions from the data cube are removed.\\n\\nDrill-down\\nDrill-down is the reverse operation of roll-up. It is performed in the following ways −\\n• By stepping down a concept hierarchy for a dimension\\n• By introducing a new dimension.\\n• Drill-down is performed by stepping down to the concept hierarchy for the dimension time.\\n• Initially, the concept hierarchy was \"day < month < quarter < year.\"\\n• On drilling down, the time dimension is descended from the level of the quarter to the level of the month.\\n• When drill-down is performed by one or more dimensions from the data cube were added.\\n•Drill down will navigate the data from less detailed data to highly detailed data.\\n\\nSlice\\nThe slice operation selects one particular dimension from the given cube and provides a new sub-cube.\\n• Here the Slice is performed by the dimension \"time\" using the criterion time = \"Q1\".\\n• It will form a new sub-cube by selecting one or more dimensions.\\n\\nDice\\nDice selects two or more dimensions from a given cube and provides a new sub-cube. \\nThe dice operation on the cube based on the following selection criteria involves in three dimensions.\\n• (location = \"Toronto\" or \"Vancouver\")\\n• (time = \"Q1\" or \"Q2\")\\n• (item =\" Mobile\" or \"Modem\") Pivot\\nThe pivot operation is also known as rotation. It rotates the data axes in order to view and provide an alternative presentation of data. we Consider the following diagram that shows the pivot operation.\\n\\nOLTP\\nOLTP (online transaction processing) is a class of software program it can capable of supporting transaction-oriented applications on the Internet.\\nIn OLTP systems are used for order entry, financial transactions, customer relationship management (CRM) and retail sales. Such systems have a large number of users who conduct short transactions. Database queries are simple, it requires sub-second response times and returns relatively few records.\\nAn important attribute of an OLTP system is its ability to maintain concurrency to avoid single points of failure in OLTP systems are often decentralized.\\n\\nApplication in Data Warehousing\\nA data warehouse helps business executives to organize, analyze and use their data for decision making.\\nA data warehouse serves as a sole part of a plan-execute-assess and \"closed-loop\" feedback system for the enterprise management.\\nThere are many applications of data warehouse. Here we are going to see 12 applications.\\nBanking Industry\\nIn the banking industry, the concentration is mainly given to the risk management and policy reversal as well as analyzing consumer data and market trends.\\nMore concentration is given for government regulations and financial decision making.\\nAlmost every bank uses the warehouse to deal with the available resource in an effective manner.\\nSome of them uses it for market research, some uses it for perform analysis of products.\\nCertain bank utilize them for exchange rates and for the development of marketing programs.\\nIn addition to that, by analyzing the transactions, spending patterns and merchant type of the card holder, the bank has the opportunity to introduce special offer and deals based on the cardholder’s activity.\\nFinance Industry\\nSimilar to banking industry, in finance industry it helps in analyzing the evaluation and trends of the spending nature of the customer which aids in maximizing the profit earned by their client.\\nConsumer Goods Industry\\nThey are used for the prediction about the consumer shopping trends, inventory management and advertisement research.\\nAn in-depth analysis between the sales and the production can be carried out and how the information is exchanged between the business partners and clients.\\nGovernment and Education\\nThe central government uses the data warehouse for research in compliance, the state government uses it for services related to human resources like recruitment and financial accounting like payroll management.\\nThe government can also use the data warehouse to maintain and analyze the tax records, health policy records and their relevant providers.\\nThe government’s data warehouse is connected with the entire criminal law database.\\nThe criminal activities can be predicted from the patterns and trends, and results of the analysis of historical data associated with the past criminals.\\nUniversities can use the data warehouse for extracting the information used for the proposal of research grants, understanding their student demographics, and human resource management.\\nIn most universities, the entire financial and financial aid departments fully depend on the data warehouses. \\nHealthcare\\nOne of the most important sectors that using the data warehouse is the health care sector.\\nTheir financial, clinical and employee records are entered into the warehouse and help them to categorize the predict outcomes and analyze their service feedback.\\nIt helps to generate the patient reports and share the data with the insurance company and medical aid services.\\nHospitality Industry\\nMajority of the hospitality industry is dominated by hotels and restaurant services, vehicle renting services and holiday resort services.\\nThey uses the data warehouse to plan and execute their promotional and advertising campaigns where they target the customers based on their feedback and travel patterns.\\nInsurance\\nAccording to the popular saying, “Insurance can never be bought and it can only be sold”.\\nThe data warehouses are mainly used to analyze the patterns of the customer mentality, apart from the record history of the existing participants.\\nWith the help of warehouse data, specially made offers and promotions for the customers can be designed.\\nManufacturing and Distribution Industry\\nThe manufacturing and distribution industry is one of the main sources of income for any state.\\nA manufacturing company has to take many make or buy decisions which influences the future growth and development of the sector.\\nSo they are using the high end OLAP tools as data warehouse to forecast the changes in the market, analyze the current trends in business and to detect warning conditions, view marketing developments and to take better decisions.\\nThey also use the warehouse data for product shipment records, products portfolios, identify profitable product lines, and analyze previous data and customer feedback to assess the unwanted and low profit product lines and to remove them.\\nThe Retailers\\nRetailers are the intermediate person between the consumers and producers.\\nIt is very much important for them to maintain records of both the consumers and the producers to ensure their survival in the market.\\nThey utilize the warehouse data to track items, their marketing and advertisings promotions and the consumers buying trends.\\nThey also analyze sales to identify the fast and slow selling products. Services Sector\\nIn the service sector, the uses of data warehouse is to maintain records such as financial, revenue patterns, profile of the customer, resource management and human resources.\\nTelephone industry\\nThe telephone industry is one of the main applications of using data warehouse. It operates over both offline and online data.\\nSo it burdens with a lot of historical data which has to be consolidated and integrated.\\nOther than these operations, analysis of fixed assets, analysis of customer calling patterns for marketing persons to push advertisements, and tracking of customer queries and complaints, the data warehouse plays a major role.\\nTransport Industry\\nThe data warehouse contains the record of customer data that enables the transport industry to experiment the target marketing which is designed as per the customers’ requirement.\\nThe internal environment of the transport industry uses the warehouse data to analyze the customer feedback, to manage the performance of teams as well as the analysis of customer financial reports for pricing strategies.\\n\\nData Mining Applications\\nToday companies with a strong focus on the customers, use data mining for retail, financial, communication, and marketing organizations, to “drill down” into their transactional data to determine pricing, customer preferences and product positioning, impact on sales, customer satisfaction and corporate profits.\\nWith the help of data mining, a retailer can use point-of-sale records of customer purchases to develop products and promotions that applies to specific customer segments.\\nFinancial Data Analysis\\nThe Financial data in banking and financial industry is generally reliable and its high quality which facilitates systematic data analysis and data mining.\\nFew of the typical cases are as follows –\\n• Design and construction of data warehouses for multidimensional data analysis and data mining.\\n• Loan payment prediction and the customer credit policy analysis.\\n• Classification and clustering of customers in targeted marketing.\\n• Detection of money laundering and other financial crimes.\\nRetail Industry\\nData Mining has its great application in retail Industry because it collects a large volume of data from its sales, customer purchase history, goods transportation, consumption and services.\\nIt is naturally that the quantity of data collected will continue to expand rapidly because of the easy, increasing availability and popularity of the web.\\nData mining helps in identifying the customer buying patterns and latest trends that lead to improve the quality of customer service and good customer retention and satisfaction.\\nHere are some examples of data mining in the retail industry –\\n• Design and Construction of data warehouses based on the benefits of data mining.\\n• Multidimensional analysis of sales, customers, products, time and region.\\n• Analysis of effectiveness in sales campaigns.\\n• Customer Retention.\\n• Product recommendation and cross-referencing of items. \\nTelecommunication Industry\\nIn today’s world the telecommunication industry is one of the most emerging and popular industries to provide various services such as fax, pager, cellular phone, internet messenger, images, e-mail, web data transmission, etc.\\nDue to the development of new communication technologies, the telecommunication industry is rapidly expanding.\\nBecause of this why data mining is become very important to help and understand the business.\\nData mining in telecommunication industry helps to identifying the telecommunication patterns, catch fraudulent activities, make better use of resource, and improve quality of service.\\n\\nData mining improves telecommunication services as follows –\\n• Multidimensional Analysis of Telecommunication data.\\n• Fraudulent pattern analysis.\\n• Identification of unusual patterns.\\n• Multidimensional association and sequential patterns analysis.\\n• Mobile Telecommunication services.\\n• Use of visualization tools in telecommunication data analysis.\\n\\nBiological Data Analysis\\nNow a days, we have seen a tremendous growth in the fields of biology such as genomics, proteomics, functional genomics and biomedical research.\\nBiological data mining is a very important part of Bioinformatics. Following are the aspects in which data mining contributes for biological data analysis –\\n• Semantic integration of heterogeneous, distributed genomic and proteomic databases.\\n• Alignment, indexing, similarity search and comparative analysis multiple nucleotide sequences.\\n• Discovery of structural patterns and analysis of genetic networks and protein pathways.\\n• Association and path analysis.\\n• Visualization tools in genetic data analysis.\\n \\nIntrusion Detection\\nIntrusion\\tdetection\\trefers\\tto\\tany\\tform\\tof\\taction\\tthat\\tthreatens\\tintegrity, confidentiality, or the availability of network resources.\\nIn the world of networking and connectivity, security has become the major issue.\\nWith the increased usage of internet and availability of the tools and tricks for intruding and attacking network prompted intrusion detection to become a critical component of network administration.\\nHere is the list of areas in which data mining technology applied for intrusion detection –\\n• Development of data mining algorithm for intrusion detection.\\n•Association and correlation analysis, aggregation to help select and build discriminating attributes.\\n• Analysis of Stream data.\\n• Distributed data mining.\\n•Visualization and query tools.\\n\\nFuture Healthcare\\nData mining holds a great potential to improve health systems.\\nIt uses the data and analytics to identify best practices to improve health care and reduce costs.\\nResearchers use data mining approaches like multi-dimensional databases, machine learning, soft computing, data visualization and statistics.\\nMining is used to predict the volume of patients in every category. Processes development will make sure that the patients receive appropriate care at the right place and right time.\\n\\nMarket Basket Analysis\\nIt is a modelling technique based upon a theory that, if you are buying a certain group of items you are likely to buy another group of items.\\nThis technique may allow the retailer to understand the purchase behaviour of a buyers.\\nThis information will help the retailer to know the buyer’s needs and change the store’s layout accordingly.\\nDifferent analysis comparison of results between different stores, between different customers in various demographic groups can be done.\\n \\nEducation\\nThere is a new emerging field called educational data mining, concerns with the developing methods that discover knowledge from data originating from educational environments.\\nThe main goals of EDM are identified as predicting students’ future learning behaviour, studying the effects of educational support, and advancing scientific knowledge about learning.\\nIt is used by institution to take the accurate decisions and to predict the results of the student.\\nWith these results the institution can focus on what to teach and how to teach the students.\\nLearning pattern of the students can be captured and used to develop techniques to teach them.\\n\\nCRM\\nCustomer Relationship Management is all about acquiring and retaining customers, also improving customer’s loyalty and implementing customer focused strategies.\\nTo maintain a proper relationship with a valuable customer a business need to collect data and analyze the information.\\nWith data mining technologies, the collected data can be used for analysis. Fraud Detection\\nThe traditional methods of fraud detection are time consuming and very hard.\\nData mining helps in providing meaningful patterns and turning data into information.\\nAny information that is valid and useful in knowledge. A perfect fraud detection system should protect information of all the users.\\nA supervised method includes a collection of sample records of classified fraudulent or non-fraudulent.\\nA model built using this data and the algorithm is made to identify whether the record is fraudulent or not.\\n \\nResearch Analysis\\nData mining is helpful in data cleaning, data pre-processing and integration of databases.\\nThe researchers can find any similar data from the database that bring any change in the research.\\nIdentification of any co-occurring sequences and the correlation between any activities can be understood. Data visualization provide us with a clear view of the data.\\n\\nCriminal Investigation\\nCriminology is a process that identifies the crime characteristics. Actually, crime analysis includes exploring and detecting crimes and their relationships with criminals.\\nThe high volume of crime datasets and the complexity of relationships between these kinds of data have made criminology an appropriate field for applying data mining techniques.\\nText based crime reports can be converted into word processing files. These information can be used to perform crime-matching process.\\n\\nConclusion\\nThere is a huge amount of data available in the Information Industry. This data is of no use until it converted into useful information. It is necessary to analyze this huge amount of data and extract useful information from it.\\nExtraction of information is not the only process we need to perform; data mining also involves other processes such as Data Cleaning, Data Integration, Data Transformation, Pattern Evaluation and Data Presentation.\\nOnce all these processes are done, we would be able to use this information in many applications.\\n\\nData Warehouse Architecture\\nA data warehouse is constructed by integrating data from multiple heterogeneous sources. It supports analytical reporting, structured and/or ad hoc queries and decision making. Some data warehouse may character reference finite set of source data, or as with most enterprise data warehouses, reference a variety of internal and external data. Because constructing a data warehouse is unique to the business use, we will look at the common layers found in all data warehouse architecture. All data warehouse architecture includes the following layers:\\n•Data Source Layer\\n•Data Staging Layer\\n•Data Storage Layer\\n•Data Presentation Layer\\nData source layer\\nThe data source layer of data warehouse architecture is where original data, collected from a variety of internal and external sources, resides in the relational database. Examples of source data types include but are not limited to: These stores disparate data types including:\\ni) Operational Data—Product data, inventory data, marketing data, or HR data.\\nii) Social Media Data—Web site hits, content popularity, content page completion.\\niii) Third-company data —Demographic data, survey data, census data.\\nWhile most data warehouse architecture deals with structured data, consideration should be given to the future use of unstructured data sources, such as voice recordings, scanned images, and unstructured text. These streams of data are valuable pieces of information and should be considered when developing the data warehouse.\\n\\nData Staging layer\\nThe data staging layer resides between data sources and the data warehouse. In this layer, data is extracted from different internal and external data sources. Because source data comes in many different formats, the data extraction layer will utilize multiple technologies and tools to extract the required data. Once the extraction of data has been completed, it will be subjected to high level data quality checks. The final result will be clean and organized data that you will load into your data warehouse. The staging layer contains the following components:\\n•Landing Database and Staging Area\\n•Data Integration Tool\\nLanding Database and Staging Area\\nThe landing database stores the information retrieved from the data source. Staging is used to apply quality checks on the information before moving it to the data warehouse. Staging is an essential step in data warehouse architecture. Poor data will amount to inadequate information and result is poor business decision making.\\n\\nData integration tool\\nExtract, Transform and Load tools (ETL) are the data integration tools used to extract data from the source system, transform and prepare data and load into the data warehouse.\\n \\nData Storage Layer\\nThe data storage layer is where data that was cleaned in the staging region is stored as a single central repository. Depending on the business and data warehouse architecture requirements, data storage may be a data warehouse, data mart (data warehouse partially replicated for specific departments), or an Operational Data Store (ODS).\\n\\nData Presentation Layer\\nThe presentation layer is where users interact with the cleansed and organized data. This layer of the data warehouse architecture provides users with the ability to query the data for the product or service insights, analyze the information to conduct hypothetical business scenarios, and develop the automated or ad-hoc study. An OLAP or reporting tool with a user-friendly Graphical User Interface (GUI) can be employed to help users to build their queries, perform analysis, or design their reports. When planning your data warehouse, create one that will handle both structured and unstructured data and is cross-functional. It should also provide a long-term foundation for data provision and decision support.\\n\\nBusiness Analytic framework\\nThe business analyst gets the information from the data warehouses to measure the performance and make critical adjustments in order to win over other business holders in the market. The data warehouse offers the following advantages.\\n• Since a data warehouse can gather information quickly and efficiently, it can enhance business productivity.\\n• A data warehouse provides us a consistent view of customers and items; hence, it helps us manage customer relationship.\\n•A data warehouse also helps in bringing down the costs by tracking trends, patterns over a long period in a consistent and reliable manner.\\n \\nTo design an effective and efficient data warehouse, we need to understand and analyze the business needs and construct a business analysis framework. Each person has different views regarding the pattern of a data warehouse. These views are as follows.\\nThe top-down view − This view allows the selection of relevant information needed for a data warehouse.\\nThe data source view − This view presents the information being captured, stored, and managed by the operational system.\\nThe data warehouse view − This view includes the fact tables and dimension tables. It represents the information stored inside the data warehouse.\\nThe business query view − It is the view of the data from the viewpoint of the end-user. \\n\\nThree-tier architecture\\nA data warehouse adopts a three-tier architecture. Following are the three tiers of the data warehouse architecture.\\nBottom Tier − The bottom tier of the architecture is the data warehouse database server.\\nIt is the relational database system. We use the back end tools and utilities to feed data into the bottom tier. These back end tools and utilities perform the Extract, Clean, Load, and refresh functions.\\nMiddle Tier − In the middle tier, we have the OLAP Server that can be implemented in either of the following ways.\\nBy Relational OLAP (ROLAP), which is an extended relational database management system. The ROLAP maps the operation on multidimensional data to standard relational operations.\\nBy Multidimensional OLAP (MOLAP) model, which directly implement the multidimensional data and operations.\\nTop -Tier − This tier is the front-end client layer. This layer holds the query tools and reporting tools, analysis tools and data mining tools.\\n\\nData Warehouse Models\\nFrom the perspective of data warehouse architecture, the following are the data warehouse models. • Virtual Warehouse • Data mart • Enterprise Warehouse Virtual Warehouse\\nThe view over an operational data warehouse is known as a virtual warehouse. It is easy to build a virtual warehouse. Building a virtual warehouse requires excess capacity on operational database servers.\\n\\nData Mart\\nData mart contains a subset of organization-wide data. This subset of data is valuable to specific groups of an organization. Data marts contain data specific to a particular group. For example, the marketing data mart may contain data related to items, customers, and sales. Data marts are confined to subjects.\\n\\nThe points about data marts are :\\nWindows-based or Unix/Linux-based servers are used to implement data marts. They are implemented on low-monetary value servers. The implementation data mart cycles is measured in short periods of time, i.e., in weeks rather than the month or years . The life cycle of a data mart may be complex in the long run, if its planning and design are not organization-wide. Data marts are small in size. Data marts are customized by the departments. The source of a data mart is departmentally structured data warehouse. Data mart are flexible.\\nEnterprise Warehouse\\nAn enterprise warehouse collects all the information and the subjects spanning an entire system. It provides us enterprise-wide data integration. The data is integrated from operational systems and external information providers. This information can vary from a few gigabytes to hundreds of gigabytes, tera bytes or beyond.\\n\\nLoad Manager\\nThis component performs the operations required to extract and load process.\\nThe size and complexity of the load manager vary between specific solutions from one data warehouse to other.\\nLoad Manager Architecture\\nThe load manager performs the following functions.\\n•\\tExtract the data from the source system.\\n•\\tFast Load the extracted data into a temporary data store.\\n•\\tPerforms simple transformations into structure similar to the one in the data warehouse.\\n\\nExtract Data from Source\\nThe data is extracted from the operational databases or the external information providers. Gateways is the application programs that are used to extract data. It is supported by underlying DBMS and allows client program to generate SQL to be executed at a server. Open Database Connection(ODBC), Java Database Connection (JDBC), are examples of gateway.\\n\\nFast Load\\nIn order to minimize the total load window, the data need to be load into the storage warehouse in the fastest possible time. Transformations affect the speed of data processing. It is more effective to load the data into the relational database prior to applying Transformations and checks. Gateway technology proves to be not suitable since they tend not to be performant when large data volumes are involved.\\n\\nSimple Transformation\\nWhile loading, it may be required to perform simple transformations. After this has been completed we are in a position to do the complex checks. Suppose we are loading the EPOS sales transaction we need to perform the following checks:\\n•\\tStrip out all the columns that are not required within the warehouse.\\n•\\tConvert all the values to the required data types.\\n\\nWarehouse manager\\nA warehouse manager is responsible for the warehouse management process. It consists of third-party system software, C program, and shell scripts. The size and complexity of warehouse managers vary between specific solutions.\\n\\nWarehouse Manager Architecture\\nA warehouse manager includes the following components.\\ni)\\tControlling process\\nii)\\tStored procedures or C with SQL\\niii)\\tBackup /Recovery tool\\niv)\\tSQL Scripts\\n\\nOperations Performed by Storage warehouse Manager are:\\ni)\\tA warehouse manager analyzes the information to perform consistency and referential integrity check.\\nii)\\tCreates index, business views, partition views against the base data.\\niii)\\tGenerates new aggregation and updates existing aggregations. Generates normalizations.\\niv)\\tTransforms and merges the source data into the published data warehouse. Backup the data in the data warehouse. Archives the data that has reached the end of its captured life.\\nA warehouse Manager also analyzes query profiles to determine index and aggregations are appropriate.\\n\\nQuery Manager\\nQuery manager is responsible for directing the queries to the suitable tables.\\nBy directing the queries to appropriate tables, the speed of querying and response generation can be increased. Query manager is responsible for scheduling the execution of the queries posed by the user.\\n\\nQuery Manager Architecture\\nThe following screenshot shows the computer architecture of a query manager. It includes the following: i) Query redirection via C tool or RDBMS ii) Stored procedures iii) Query management tool iv) Query scheduling via third party software v) Query scheduling via third-party software.\\n\\nDetailed Information\\nDetailed information is not kept online, rather it is aggregated to the next level of detail and then archived to tape. The detailed information part of data warehouse keeps the detailed information in the starflake schema. Detailed information is loaded into the data warehouse to supplement the aggregated data.\\nIf detailed information is held offline to minimize disk storage, we should make sure that the data has been extracted, cleaned up, and transformed into starflake schema before it is archived.\\n\\nSummary Information\\nSummary Information is a part of data warehouse that stores predefined aggregations. These aggregations are generated by the warehouse manager. Summary Information must be treated as transient. It changes on-the-go in order to respond to the changing query profiles.\\n\\nThe points about summary information are:\\n• Summary information speeds up the performance of common queries.\\n• It increases the operational cost.\\n• It needs to be updated whenever new data is loaded into the data warehouse.\\n• It may not have been backed up, since it can be generated fresh from the detailed information.\\n\\nConclusion\\n• A data warehouse is constructed by integrating data from multiple heterogeneous sources. It supports analytical reporting, structured and/or ad hoc queries and decision making.\\n• A data warehouse is subject oriented as it offers information regarding a theme instead of companies\\' ongoing operations. These subjects can be sales, marketing, distributions, etc.\\n• There are 5 main components of a Data warehouse. 1) Database 2) ETL Tools 3) Meta Data 4) Query Tools 5) Data Marts.\\n• The four main categories of query tools 1. Query and reporting, tools 2. Application Development tools, 3. Data mining tools 4. OLAP tools\\n• In the Data Warehouse Architecture, meta-data plays an important role as it specifies the source, usage, values, and features of data warehouse data.\\n\\nData Warehouse Architecture II Reference Architecture\\nA reference structure in the area of software program architecture or organization structure gives a template answer for an architecture for a particular domain. It offers you a common understanding of the names and layers.\\nBefore explaining the photograph let me shortly outline the abbreviations: CDW: Core Data Warehouse. Also known as EDW (Enterprise Data Warehouse)\\nThere are two sorts of Data Warehouse architectures that I typical see and suggest to customers:\\nSingle Data Warehouse: A single Data Warehouse solely has one organization vast statistics mart on top of the CDW.\\nMulti-Mart Data Warehouse: A multi-mart Data Warehouse has more that one Data Mart on top of the CDW. This plan I normally see at surely large clients the place it doesn’t make any feel to push all the statistics only in one single Data Mart. A exact example is a separated Data Mart for international monetary reporting, one for production analytics and another one for provide chain optimization.\\n\\nHow many layers and for which purpose?\\nFirst of all, every layer described in Data Warehouse architecture ought to have a positive reason to be there. The definition of layers is a conscious selection of the Data Warehouse architect! No layer is just there by way of accident.\\nSo let me outline my layers and the purpose of every layer:\\n\\nStaging layer\\nThe staging layer helps 1:1 reproduction of supply system extraction. The most important aim is to convey the facts as quick and as easy and viable from the sources to the SQL Server additionally in order to reduce the supply system interaction. Data in the staging location is transient or semi-temporary and can be deleted after all information is loaded into the CDW and the archive.\\n \\nStaging Area – Design Principles\\nThe following list of plan rules follows for the Staging area:\\nSame naming conventions and data types as the source system \\nNo transformations\\nNo aggregations \\nNo fine checks\\n\\n1 Staging location per source system\\nThe first five concepts center of attention all on the concept of bringing the information as convenient and as fast as possible into the staging layer of the Data Warehouse in order to reap clean connectivity to our sources.\\nWe additionally prefer to preserve the extraction workload as brief as possible in order to minimize the load of outsourcing systems. An ERP system like SAP ERP is normally mission vital in a client employer surroundings so we favor to limit the statistics extraction affect on the day by day business. Therefore we continually prefer for a delta extraction process, if the source system is in a position to deliver modifications only.\\nThe factor of having one staging area per source device is because of workload and records separation. This design relies upon a little bit on the flavor of the architect and also on the different numbers and complexities of the supply systems. Of course, it is additionally viable to separate source system in the staging area through defining specific schemas per source gadget in large corporation environments a committed database per supply is greater common.\\n\\nStaging Area – SSIS Loading Pattern\\nBecause we have no addition logic to implement at some point of facts supply extraction the SSIS applications look very simple.\\nWe use a source machine adapter (the one that suits fantastic for the unique source system connectivity), in this case an OleDB source, extract the data, add some audit data like CreatedDate or LoadID inside a Derived Column task and store the data in our staging table.\\n\\nThis appears very easy, isn’t it? And exactly this was once the intention for staging area information loads.\\nArchive: The archive layer is our “life insurance”. Why? Because it maintains the full data history. the Data Warehouse has to track the complete history. That capability that every single attribute trade would have been tracked in the CDW layer.\\n This is very expensive, particularly if only for around 10% of the attribute records tracking is of interest for business person analysis. So why to track 100% in the CDW and no person use it? Does that make sense? I assume no, however in order to be flexible, because we don’t be aware of which attribute history is of interest in the future, we track the full records in our archive.\\n\\nArchive – The Purpose\\nLets know about “what is data history” first. There are two sorts of records tracking in a Data Warehouse.\\nFact Data Changes: This is the easy one. Because every reality report has a link to a sure point in time our facts are records tracked automatically.\\nMaster Data Changes: This is the difficult one and is additionally comprehend as slowly changing dimensions (SCD). In this case we track attribute changes in our dimension like telephone number or address information.\\n\\nWhen I say I recommend no longer to track the full records inside the Data Warehouse I mean now not to track all attribute adjustments that manifest simply in advance. Based on my journey solely 10% of history tracked dimension attributes are used with the aid of enterprise users. So why I then track the extra 90%, if no person use them?\\nMost of the time I hear the answer: “Because we don’t recognize what will be used in the future.”\\nBut do you genuinely prefer to pay the rate for that? And the charge would be lot’s of between joins in your Data Warehouse and between joins are very fee sizable operations. I will dig into that subject in my submit about the Core Data Warehouse design.\\nTo make it short, we have the full records (fact and master data) in our archive and are always able to create a new SCD2 table in our Core Data Warehouse when ever needed.\\n\\nArchive – Design Principles\\nThe archive follows the same design rule as the Scaffolding field plus some additional principles: 1:1 copy of the staging area Same naming conventions and data type as the staging field No transformation No assembling No quality checks 1 Archive per Staging area Use Compression and Partitioning Apply delta detection if necessary\\nIf the reference systems are not able to deliver data changes only and instead give you a full extract of their data every clip I would add some logic before loading that data to the archive. In case of full excerption, your archive will grow very rapidly so you need to add a delta detection logic between staging and archive and loading only data changes into the archive.\\nI also would apply tabular array compression and partition. Partitioning in case you need to load a certain extract again from the archive to your Data Warehouse. Archive\\n– SSIS Loading pattern\\nBecause we have no further logic to implement during the SSIS software look very simple.\\nWe use OleDB adapter to attach to our Staging data, extract the info, add some audit data like CreatedDate or LoadID inside a Derived Column task and store the info in our Archive table.\\nAgain a simple and straight forward approach.\\n\\nCDW: This layer is that the relative information Warehouse layer and since it is used for business analysis it\\'s modules as star schema however an extra terribly import purpose of this layer is quick ETL.\\n\\nIf the analysis necessities obtaining more durable and you wish to optimize the info model for reportage (aggregation, indexes) so as to meet business user necessities, you wish to push it to the info sales outlet. Otherwise, your whole ETL method would suffer from performance and optimize a layer for each (ETL and reporting) may be a very hard job. The CDW isn\\'t designed as a mixture of all information marts. The goal of the CDW is to represent the full enterprise of a client at rock bottom roughness if possible.\\n\\nData Marts vs. Centralized Data Warehouse: Use Cases\\nThe following use cases highlight some examples of when to use each approach to data warehousing.\\nData Marts Use Cases\\n• Marketing analysis and reporting favor a data mart approach because these activities are typically performed in a specialized business unit, and do not require enterprise-wide data\\n• A financial analyst can use a finance data mart to carry out financial reporting.\\n\\nCentralized Data Warehouse Use Cases\\nA company considering an expansion needs to incorporate data from a variety of data sources across the administration to come to an informed decision. This requires a data warehouse that aggregates data from sales, selling, computer store direction, customer loyalty, supply chain, etc.\\nMany factors private road profitableness at an insurance company. An insurance company reporting on its gain needs a centralized data warehouse to combine entropy from its claim department, sales, customer demographics, investments, and other domain.\\n\\nData Mart: Designed as an optional layer above the CDW the Data Mart data model is also designed as a star schema. But there needs to be a certain intellect to have an additional star schema on top of the CDW. What can this be? Aggregation, dimension history usage, business logic, strong requirements for relational reporting, As already said if the requirements for relational reporting are hard to handle in the CDW layer, thrust them to the Data Mart.\\n\\nData Mart vs. Data Warehouse\\nA information mart is a subset of a data warehouse oriented to a specific business line. Data marts contain repositories of summarized data collected for analysis on a specific department or unit within an organization, for example, the sales department.\\nA data warehouse is a large centralized repository of data that contains info from many source within an organization. The collated data is used to guide business decisions through analysis, reporting, and data mining tools.\\nData Mart and Data Warehouse Comparison Data Mart\\n• Focus: A single subject or functional organization area\\n• Data Sources: Relatively few sources linked to one line of business\\n• Size: Less than 100 GB\\n• Normalization: No preference between a normalized and de-normalized structure\\n• Decision Types: Tactical decisions pertaining to particular business lines and ways of doing things\\n• Cost: Typically from $10,000 upwards\\n• Setup Time: 3-6 months\\n• Data Held: Typically summarized data Data Warehouse\\n• Focus: Enterprise-wide repository of disparate data sources\\n• Data Sources: Many external and internal sources from different areas of an organization\\n• Size: 100 GB minimum but often in the range of terabytes for large organizations\\n• Normalization: Modern warehouses are mostly denormalized for quicker data querying and read performance\\n• Decision Types: Strategic decisions that affect the entire enterprise\\n• Cost: Varies but often greater than $100,000; for cloud solutions costs can be dramatically lower as organizations pay per use\\n• Setup Time: At least a year for on-premise warehouses; cloud data warehouses are much quicker to set up\\n• Data Held: Raw data, metadata, and summary data\\n• Multidimensional Data Mart: The multidimensional layer, typically implemented with Analysis Services, is also an optional layer on top of the relational Data Mart or the CDW. Which technology to use in case of reporting depends always on the business user reporting requirements.\\n\\nWhat is the ETL Process?\\nExtract, Transform, Load (ETL), an automated mental process which takes.\\n1) Raw data formation,\\n2) Extract the data required for analytic thinking,\\n3) Transforms it into a format that can serve business needs, and\\n4) Loads it to a data warehouse.\\n\\nETL typically summarizes data to reduce its size and improve performance for particular type of analysis. When you build an ETL substructure, you must integrate data sources, and carefully plan and test to ensure you transform source data correctly. \\nBelow we explain three senses of way to form an ETL infrastructure and one more way to build a data pipeline without using ETL at all.\\nTwo Ways to Build An ETL Process\\n1. Building an ETL Pipeline with Batch Processing\\n2. Building a Pipeline without ETL Using an Automated Cloud-Based Data Warehouse Building an ETL Pipeline with Batch Processing\\nFollow the process below to build a traditional ETL process, in which you transportation and process information in batch from source information bases to the data warehouse. It’s challenging to build an enterprise ETL pipeline from scratch - you will typically rely on ETL creature such as Stitch or Blendo, which simplify and automate much of the process. Building ETL with batch processing, following ETL best pattern, involves:\\nReference data - create a lot of data that defines the set of permissible values your data may contain. For example, in a country data field, you can define the list of country computer code allowed.\\nExtract from data source - the basis for the success of subsequent ETL steps is to do selection of data correctly. Most ETL combine data from multiple source scheme, each with its own data organization and data format - including relational databases, non- relational databases, XML, JSON, CSV file cabinet, etc. Successful origins convert data into a single format for standardized processing.\\nData validation - an automated process confirms whether data pulled from root has the expected values - for example, in a database of financial transactions from the past year, a date field should contain valid particular date within the past twelve months. The validation rejects data if it fails the validation rule. You analyze rejected records, on an ongoing basis, to identify what went wrong, correct the source data, or modify extraction to resolve the problem in the next batches.\\nTransform data - removing extraneous or erroneous data (cleanup), applying concern rules, checking data integrity (ensuring that the data was not corrupted in the source, or corrupted by ETL and that no data was dropped in previous stages), and creating aggregates as necessary.\\nFor example, if you must analyze tax revenue, you can summarize the dollar amount of bill into a daily or monthly total. You will need to program and exam a series of rules or use that can achieve the required transformations, and run them on the extracted data.\\n\\nBuilding a Pipeline without ETL Using an Automated Cloud-Based Data Warehouse\\nNew cloud-based information warehouse technology makes it possible to achieve the original goal of ETL without building an ETL system at all.\\nFor example, Panoply’s cloud-based automated information warehouse has close end- to-end information direction anatomy. It uses a self-optimizing architecture with machine learning and natural language processing (NLP), which automatically extracts and transforms information to match analytics demand.\\nPanoply comes pre-integrated with dozens of data sources, including analytics systems, Bi tools, databases, mixer and advertising platforms. Building a data pipeline without ETL in Panoply involves:\\nSelect data sources and import data - select your data sources from a list, enter certificate and define name and address tables, click Collect and Panoply automatically drag the data for you.\\n\\nRun transformation queries –\\nSelect a table and test a SQL query against the raw data formation. You can save the query as a transformation, or export the resulting table into your own system. Panoply supports both simple views and materialized transformation views.\\nYou can run several transformations until you achieve a data format that enables depth psychology. Panoply’s Interrogation Logarithm allows you to easily coil back to previous processing stone\\'s throw.\\nYou shouldn’t be concerned about “ruining” the data - Panoply lets you perform any transformation, but keeps your raw data intact.\\nData analysis with Bi tools - you can now connect any Bi tool such as Tableau or Ravisher to Panoply and explore the transformed data.\\nThe above process is agile and flexible, allowing you to quickly load data, transform it into a useful form, and use it to perform business analysis.\\n\\nConclusion\\nA multidimensional data warehouse is typically used for the design of corporate data warehouse and departmental data marts. Such a model can adopt a star schema, snowflake schema or fact constellation.\\nData warehouse is an information system that contains historical and commutative data from single or multiple sources.\\nA data warehouse is subject oriented as it offers information regarding subject instead of organization\\'s ongoing operations.\\nIn Data Warehouse, integration means the establishment of a common unit of measure for all similar data from the different databases\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert into lower and sentence tokenizer\n",
        "data=data.lower()\n",
        "sentoken=nltk.sent_tokenize(data)\n",
        "wordtoken=nltk.word_tokenize(data)\n",
        "sentoken[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk1gG07IGmwT",
        "outputId": "8056d50f-0aed-4674-f9ea-11d31bf62af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data mining is the analysis step of knowledge discovery in databases process, or kdd.',\n",
              " 'data mining is the extraction of hidden predictive information from large databases is a new technology with great potential to help companies focus on the most important information in their data warehouses.',\n",
              " '–\\tit’s non-trivial extraction of implicit, previously unknown and potentially useful information from data\\n–\\texploration & analysis,by automatic orsemi-automatic means, of large quantities of data in order to discover meaningful patterns\\n\\n\\ndata mining is defined as the procedure to extracting information from huge sets of data.',\n",
              " 'we can say that data mining is mining knowledge from data.',\n",
              " 'huge volume of data is available in the information industry.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenizer\n",
        "wordtoken[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwcnTiHaHX58",
        "outputId": "0624f91d-bb20-454a-c5a4-84c2909168b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'mining',\n",
              " 'is',\n",
              " 'the',\n",
              " 'analysis',\n",
              " 'step',\n",
              " 'of',\n",
              " 'knowledge',\n",
              " 'discovery',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using WordnetLemmatizer lemmadation, remove punctions on data and save as tokens\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def Lemtoken(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "puncdict=dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return Lemtoken(nltk.word_tokenize(text.lower().translate(puncdict)))\n"
      ],
      "metadata": {
        "id": "Wc6yzKZkIcJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greeting responses from chatbot define a function\n",
        "greetinput=('hello','hii','chatbot','hey','whatsapp','how are you')\n",
        "greetresponse=('hello','hi','welcome','heyy','good start with you today','you made my day bright!! how can i help you')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greetinput:\n",
        "      return random.choice(greetresponse)\n",
        "print (greet('chatbot hii'))"
      ],
      "metadata": {
        "id": "8g8bCJpVKxFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f393a488-78af-44b4-ab4b-e330f257566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you made my day bright!! how can i help you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "xbxXofsUM0-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define response function to generate response for chatbot\n",
        "def response(userresponse):\n",
        "  robolresponse= ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english') # tokeniztion function, remove stop words\n",
        "  tfidf = TfidfVec.fit_transform(sentoken) # calculate termfrequency\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf) # and cosine similarity of term frequency\n",
        "  idx = vals.argsort()[0][-2] #binding most similar or the first elements\n",
        "  flat = vals.flatten() # flatten the values\n",
        "  flat.sort()\n",
        "  reqtfidf = flat[-2]\n",
        "  if (reqtfidf == 0): # if the user entered word not in the database then it returns unable to understand\n",
        "    robolresponse = robolresponse + \"I am sorry. Unable to understand you\"\n",
        "    return robolresponse\n",
        "  else:\n",
        "    robolresponse = robolresponse + sentoken[idx]\n",
        "    return robolresponse"
      ],
      "metadata": {
        "id": "mQaondvKNXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"Hello!! start with greeting and end with 'bye',How can i help you with today\")\n",
        "while (flag == True):\n",
        "  userresponse = input(\"User :\")\n",
        "  userresponse = userresponse.lower()\n",
        "  if (userresponse != 'bye'):\n",
        "    if (userresponse == 'thank you' or userresponse == 'thanks'):\n",
        "      flag = False\n",
        "      print('Bot: you are welcome..')\n",
        "    else:\n",
        "      if(greet(userresponse) != None):\n",
        "        print('Bot ' + greet(userresponse))\n",
        "      else:\n",
        "        sentoken.append(userresponse)\n",
        "        wordtoken = wordtoken + nltk.word_tokenize(userresponse)\n",
        "        finalwords = list(set(wordtoken))\n",
        "        print('Bot: ', end = '')\n",
        "        print(response(userresponse))\n",
        "        sentoken.remove(userresponse)\n",
        "  else:\n",
        "    flag = False\n",
        "    print('Bot: Goodbye! have nice day!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQVcskPINW4X",
        "outputId": "b9b6b85b-6880-4dbd-f6bc-1f2bf0494a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!! start with greeting and end with 'bye',How can i help you with today\n",
            "User :hii\n",
            "Bot you made my day bright!! how can i help you\n",
            "User :data warehouse\n",
            "Bot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the transformation of data in the data warehouse is done to meet the objectives of data warehouse\n",
            "real time data warehouse:\n",
            "during this stage every data warehouses are refreshed and updated during a transaction.\n",
            "User :data\n",
            "Bot: a common source for data is a data mart or data warehouse.\n",
            "User :kdd\n",
            "Bot: data mining is the analysis step of knowledge discovery in databases process, or kdd.\n",
            "User :thanks\n",
            "Bot: you are welcome..\n"
          ]
        }
      ]
    }
  ]
}